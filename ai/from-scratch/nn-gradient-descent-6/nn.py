# Learning is adjusting the weight to reduce the error to 0
# Sensitivity between weight and error

# Derivative, Wikipedia:
# The derivative of a function of a real variable measures the sensitivity to change of the
# function value (output value) with respect to a change in its argument (input value)

# The *derivative* of the position of a moving object with respect to time, is the object's velocity
# This measures how quickly the position of the object changes when time advances.

# position and time, the derivative is velocity

# red_length = blue_length * 2
# The derivative is 2

# You always have the derivative between two variables

# If the derivative is positive then when you change one variable,
# the other will move in the same direction

# If the derivative is negative, then when you change one variable,
# the other will move in the opposite direction

# Derivative is the slope at a point on a line or curve

# A Neural Network is really just one thing: a bunch of weights you use to compute an error function.

######
# weight_delta is the derivative