Word2vec is a technique for natural language processing published in 2013.
The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.
Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.
As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector.
The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level
of semantic similarity between the words represented by those vectors.
Word2vec is a group of related models that are used to produce word embeddings.
These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.
Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions,
with each unique word in the corpus being assigned a corresponding vector in the space.
Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one
another in the space.
Word2vec can utilize either of two model architectures to produce a distributed representation of words:
continuous bag-of-words (CBOW) or continuous skip-gram.
In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words.
The order of context words does not influence prediction (bag-of-words assumption).
In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.
The skip-gram architecture weighs nearby context words more heavily than more distant context words.
According to the authors' note, CBOW is faster while skip-gram does a better job for infrequent words.
