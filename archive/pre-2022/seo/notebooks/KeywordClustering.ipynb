{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "KeywordClustering",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Keyword Clustering - SEO\n",
    "\n",
    "Check out the repository for more code https://github.com/PabloRosales/code-examples\n",
    "\n",
    "You can also follow me on twitter at [@_PabloDev](https://twitter.com/_PabloDev)\n",
    "\n",
    "You can also help by supporting on Patreon: https://www.patreon.com/_pablodev\n",
    "\n",
    "We'll just use the example from SBert with some tweaks: https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/agglomerative.py"
   ],
   "metadata": {
    "id": "UxuvXbVesDuj",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install dependencies\n",
    "\n",
    "We will use Sentence Transformers \n",
    "\n",
    "Check out the documentation at https://www.sbert.net/"
   ],
   "metadata": {
    "id": "IXScgfggsnFl",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install sentence_transformers"
   ],
   "metadata": {
    "id": "CkC_0QowsDe6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download the model\n",
    "\n",
    "We'll use the biggest model `all-MiniLM-L6-v2`, you can try different models, check out their page: https://www.sbert.net/docs/pretrained_models.html"
   ],
   "metadata": {
    "id": "OFnuDt8ys5KP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder='/tmp/.cache')"
   ],
   "metadata": {
    "id": "3upJxiaHs4ZE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Other depenencies"
   ],
   "metadata": {
    "id": "XYV9pZAxt-Bn",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will do Agglomerative Clustering"
   ],
   "metadata": {
    "id": "8JIthuoyuIFy",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ],
   "metadata": {
    "id": "XhZsJKs5uBNL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also be using Pandas and Numpy"
   ],
   "metadata": {
    "id": "c42So-FWviHP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "p1cmHwZ2vjJE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seaborn will be used to graph the length of our keywords"
   ],
   "metadata": {
    "id": "r-nljt0pvyWr",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns"
   ],
   "metadata": {
    "id": "9D3hzNWAvzoe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets import IO to load our file"
   ],
   "metadata": {
    "id": "q7thsSKKwUEO",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import io"
   ],
   "metadata": {
    "id": "uhEm7y9FwTMj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add your keywords from a CSV file\n",
    "\n",
    "Your CSV file must include a **keyword** column, otherwise you'll need to adapt the code."
   ],
   "metadata": {
    "id": "HuQS-MyTuh9E",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()"
   ],
   "metadata": {
    "id": "nBAwhbiwutJU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and check our keywords\n",
    "\n",
    "The fun part, pretty simple actually with sentence transformers.\n",
    "\n",
    "But first lets grab the keywords from the CSV"
   ],
   "metadata": {
    "id": "BhkWFxaev-BP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "filename = list(uploaded.keys())[0]\n",
    "df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
    "corpus_of_keywords = df['keyword'].values"
   ],
   "metadata": {
    "id": "KXlglkxvwC1E",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets check how many keywords we have:"
   ],
   "metadata": {
    "id": "MTwSXiUBxaYx",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "len(corpus_of_keywords)"
   ],
   "metadata": {
    "id": "tx2ClAh8xYfJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets check the length of the keywords"
   ],
   "metadata": {
    "id": "9ECuPLb2xnKU",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df[\"keyword_word_len\"] = df[\"keyword\"].apply(lambda x : len(x.split()))\n",
    "df[\"keyword_len\"] = df[\"keyword\"].apply(lambda x : len(x))"
   ],
   "metadata": {
    "id": "Vbhw-QmQxmRq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "First by word count"
   ],
   "metadata": {
    "id": "cterTgp8x5C-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sns.displot(df.keyword_word_len, kde=False)"
   ],
   "metadata": {
    "id": "PlR0kPmPx6mD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now by character length"
   ],
   "metadata": {
    "id": "Ii-V-fTUx8TO",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sns.displot(df.keyword_len, kde=False)"
   ],
   "metadata": {
    "id": "bjsPBRz6yA77",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can do do some cleaning up now..."
   ],
   "metadata": {
    "id": "4JotzcuRyD80",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now lets cluster them"
   ],
   "metadata": {
    "id": "ITzZgD3fyJ_A",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets first create the embeddings:"
   ],
   "metadata": {
    "id": "rDkIu3-HyR6O",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "corpus_embeddings = model.encode(corpus_of_keywords)\n",
    "corpus_embeddings = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)"
   ],
   "metadata": {
    "id": "9m_IQnsvyM8a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can adjust some values depending on how close you want your clusters to be.\n",
    "\n",
    "A distance_threshold of 0.10 works great if you want the clusters to be very similar semantically, you can use 0.30 to get them a bit looser."
   ],
   "metadata": {
    "id": "fQD9amhqy4bJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "distance_threshold = 0.10"
   ],
   "metadata": {
    "id": "5lrZ_yGX3jKI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "PXaRLrLY3jZ8",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "clustering_model = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='average', distance_threshold=distance_threshold)\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_"
   ],
   "metadata": {
    "id": "YmDN0NDiyzbX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets cluster them:"
   ],
   "metadata": {
    "id": "LDUa3XGRzFBx",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "clustered_sentences = {}\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "    clustered_sentences[cluster_id].append(corpus_of_keywords[sentence_id])"
   ],
   "metadata": {
    "id": "wdXKtAzgzD4I",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "And finally group them by first keyword in cluster:"
   ],
   "metadata": {
    "id": "QjBOspw0zIuV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "_cluster_items = clustered_sentences.items()\n",
    "\n",
    "clusters = {'others': []}\n",
    "for cluster in _cluster_items:\n",
    "    if len(cluster[1]) == 1:\n",
    "        clusters['others'].append(cluster[1][0])\n",
    "        continue\n",
    "    clusters[cluster[1][0]] = cluster[1]"
   ],
   "metadata": {
    "id": "ggSvujhbzOSe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check how many clusters we got"
   ],
   "metadata": {
    "id": "4H1uwoOm01Kt",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "len(clusters.keys())"
   ],
   "metadata": {
    "id": "WXftpota0EAL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "keywords_with_cluster = []\n",
    "\n",
    "for cluster in clusters.keys():\n",
    "    for kw in clusters[cluster]:\n",
    "        keywords_with_cluster.append(dict(\n",
    "            keyword=kw,\n",
    "            cluster=cluster,\n",
    "        ))\n",
    "\n",
    "df_clusters = pd.DataFrame(keywords_with_cluster)"
   ],
   "metadata": {
    "id": "VAvtRlJb2qaM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_clusters"
   ],
   "metadata": {
    "id": "ySRcM9lZ3TmM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download CSV with cluster data"
   ],
   "metadata": {
    "id": "y3mHwaAJ04Lr",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_clusters.to_csv('keywords_clustered.csv', encoding = 'utf-8-sig') \n",
    "files.download('keywords_clustered.csv')"
   ],
   "metadata": {
    "id": "2hdQwT8s06s-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final notes"
   ],
   "metadata": {
    "id": "kai_s8oh10vG",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Semantic clustering is a great first step, but just because two keywords are semantically different, does not mean that Google treats them as separated intents. **I'll share another notebook for SERP clustering if asked.**"
   ],
   "metadata": {
    "id": "RMFB6RrR1_RU",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}